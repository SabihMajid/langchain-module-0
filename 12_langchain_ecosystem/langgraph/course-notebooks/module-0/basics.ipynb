{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SabihMajid/langchain-module-0/blob/main/12_langchain_ecosystem/langgraph/course-notebooks/module-0/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
      "metadata": {
        "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
      },
      "source": [
        "# LangChain Academy\n",
        "\n",
        "Welcome to LangChain Academy!\n",
        "\n",
        "## Context\n",
        "\n",
        "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible.\n",
        "\n",
        "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state.\n",
        "\n",
        "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
        "\n",
        "## Course Structure\n",
        "\n",
        "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independent of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
        "\n",
        "## Chat models\n",
        "\n",
        "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course with use [GEMINI_API_KEY](https://ai.google.dev/gemini-api/docs/api-key/) because it is both popular and performant. As noted, please ensure that you have an `GEMINI_API_KEY`.\n",
        "\n",
        "Let's check that your `GEMINI_API_KEY` is set and, if not, you will be asked to enter it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0f9a52c8",
      "metadata": {
        "id": "0f9a52c8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q TavilySearchResults\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily_python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ch7HciDZXh3F"
      },
      "id": "Ch7HciDZXh3F",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"where am I?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKnOnfj3YZYT",
        "outputId": "a3eefb4e-3dd9-4f7f-b60d-da017d5b6c5a"
      },
      "id": "KKnOnfj3YZYT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I do not know where you are. I have no access to your location.  To find out where you are, you can use a map application on your phone or computer, or look for street signs or other landmarks.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-3050c2d9-171c-423f-8626-15e8dfce2136-0', usage_metadata={'input_tokens': 5, 'output_tokens': 46, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28450d1b",
      "metadata": {
        "id": "28450d1b"
      },
      "source": [
        "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
        "\n",
        "* `stream`: stream back chunks of the response\n",
        "* `invoke`: call the chain on an input\n",
        "\n",
        "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b1280e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1280e1b",
        "outputId": "1293664d-86b4-4d3d-97d7-3c579321b438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Python is a fantastic choice for building LLM applications.  It offers a rich ecosystem of libraries specifically designed for working with large language models. Here\\'s how you can use it:\\n\\n**1. Choosing an LLM API or Library:**\\n\\n* **OpenAI API:** This is a popular choice, offering access to powerful models like GPT-3, GPT-4, and others.  You\\'ll use the `openai` Python library to interact with the API.  This involves making API calls to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\\n\\n* **Hugging Face Transformers:** This library provides access to a vast collection of pre-trained language models, including many open-source alternatives to OpenAI\\'s models.  It allows you to download and run models locally or use them via inference APIs.  This gives you more control and potentially lower costs, but requires more technical expertise to manage.\\n\\n* **Other APIs:**  There are many other LLM providers, each with its own Python library or API access.  Consider factors like cost, model capabilities, and ease of use when choosing.\\n\\n**2. Core Python Libraries:**\\n\\nBeyond the LLM-specific libraries, you\\'ll likely use these standard Python libraries:\\n\\n* **`requests`:** For making HTTP requests to APIs (especially important when using cloud-based LLMs).\\n* **`json`:** For handling JSON data, the typical format for communication with LLM APIs.\\n* **`asyncio` (or other asynchronous frameworks):**  For handling multiple API requests concurrently, improving efficiency, especially when dealing with many users or long processing times.\\n* **`Flask` or `FastAPI`:**  For building web APIs or web applications to expose your LLM functionality to users.\\n* **`streamlit` or `gradio`:** For quickly creating user interfaces for your LLM application.\\n\\n\\n**3. Example (using OpenAI API):**\\n\\nThis simple example shows how to generate text using the OpenAI API:\\n\\n```python\\nimport openai\\n\\n# Set your OpenAI API key\\nopenai.api_key = \"YOUR_API_KEY\"\\n\\nresponse = openai.Completion.create(\\n  engine=\"text-davinci-003\",  # Or another suitable model\\n  prompt=\"Write a short story about a robot learning to love.\",\\n  max_tokens=150,\\n  n=1,\\n  stop=None,\\n  temperature=0.7,\\n)\\n\\nstory = response.choices[0].text.strip()\\nprint(story)\\n```\\n\\n**4. Building a Complete Application:**\\n\\nA more complete LLM application might involve:\\n\\n* **Prompt Engineering:** Carefully crafting prompts to elicit the desired responses from the LLM.\\n* **Error Handling:**  Handling potential API errors and providing graceful user feedback.\\n* **Context Management:**  Maintaining conversation history to provide context for subsequent interactions (especially important for chatbots).\\n* **Deployment:** Deploying your application to a server (e.g., using platforms like Heroku, AWS, Google Cloud) to make it accessible to users.\\n\\n\\nIn short, Python provides the tools and flexibility to build a wide range of LLM applications, from simple text generation scripts to complex, interactive chatbots and sophisticated AI assistants.  The specific libraries and techniques you\\'ll use will depend on the complexity and features of your application.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b95ebe0f-3deb-414f-bfbb-cc6a031b258d-0', usage_metadata={'input_tokens': 55, 'output_tokens': 724, 'total_tokens': 779, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Create a message\n",
        "# Message list\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hi\", name=\"ALEX\"),\n",
        "    AIMessage(content='Hi there! How can I help you today?', name=\"AI ASSISTANT\"),\n",
        "    HumanMessage(content=\"what is python?\", name=\"ALEX\"),\n",
        "    AIMessage(content=\"Python is a high-level, general-purpose programming language.Python is used in web game development, and much more.\", name=\"AI ASSISTNT\"),\n",
        "    HumanMessage(content=\"how can i use this for llm app?\", name=\"ALEX\"),\n",
        "    ]\n",
        "\n",
        "# Invoke the model with a list of messages\n",
        "llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac73e4c",
      "metadata": {
        "id": "cac73e4c"
      },
      "source": [
        "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c0e5a",
      "metadata": {
        "id": "582c0e5a"
      },
      "source": [
        "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks.\n",
        "\n",
        "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad0069a",
      "metadata": {
        "id": "3ad0069a"
      },
      "source": [
        "## Search Tools\n",
        "\n",
        "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "52d69da9",
      "metadata": {
        "id": "52d69da9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "tavily_search = TavilySearchResults(max_results=3, api_key=TAVILY_API_KEY)\n",
        "search_docs= tavily_search.invoke(\"Where am i?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWzZ6zAvb2Fv",
        "outputId": "587f3d08-7202-4451-a94b-f7fd2bdba1d9"
      },
      "id": "JWzZ6zAvb2Fv",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.where-am-i.co/',\n",
              "  'content': 'My Location Now\\nWhere am I - Map\\nAllow current location access to proceed\\nMy Location Now\\nYou can share your current location by sending the following \"where am I\" link. Our Where am I app shows your current location on the map below or Google Maps and helps you find your coordinates and your location address.\\n If you are using an Android device, here is the link to make sure your browser has been granted access to your location: Official Google support.\\n If you are visiting this page on an iPhone or on iPad, make sure your browser has been granted access to your location: Official Apple support.\\n Where am i right now tips: for privacy reasons, please use a secure connection to show\\nmy current location on Google Maps (make sure the URL starts with https://).'},\n",
              " {'url': 'https://where-am-i.org/',\n",
              "  'content': 'The where am I latitude and longitude tool will find the lat long of your current location in case you need for your other GPS devices or share it with others.\\n Does it work on a mobile phone?\\n- Yes, the where am I tool works on any mobile device and computer as long as you have the location feature enabled on your phone.\\n Install the where am I app\\nWhere Am I | Terms | Privacy | Contact\\n©2024 Where Am I Where am I on iPhone & iPad?\\n- Yes, the where am I tool works well with any version of iPhone and iPad.\\n what country am I in - are you flying and not sure where you have landed or are you between the border of two countries?\\n'},\n",
              " {'url': 'https://www.gps-coordinates.net/my-location',\n",
              "  'content': 'My Location - What is my current location? Coordinates My Location US Map Maps My location now What is my location now? Your current location right now is shown on the Apple map below as a blue marker. You can also find out your location coordinates and your location address to answer \"where am I\". What are my coordinates? My location coordinates are: What is my location? My location address is: My Location Map How to find my location Load the \"my location\" page Allow the browser to access your location If you are using a mobile device, enable location services (GPS location sharing with your browser) for the best current location accuracy'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDrNsNteMqH6"
      },
      "id": "PDrNsNteMqH6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}